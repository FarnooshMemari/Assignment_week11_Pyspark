{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b290abf6-bc83-49f5-af9c-ca5052dec7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/databricks-datasets/COVID/CORD-19/  -->  7.00 GB\ndbfs:/databricks-datasets/airlines/  -->  120.06 GB\ndbfs:/databricks-datasets/asa/airlines/  -->  11.20 GB\ndbfs:/databricks-datasets/genomics/grch37/  -->  1.65 GB\ndbfs:/databricks-datasets/genomics/grch37_merged_vep_96/  -->  13.28 GB\ndbfs:/databricks-datasets/genomics/grch37_refseq_vep_96/  -->  10.94 GB\ndbfs:/databricks-datasets/genomics/grch37_star/  -->  27.85 GB\ndbfs:/databricks-datasets/genomics/grch37_vep/  -->  13.28 GB\ndbfs:/databricks-datasets/genomics/grch37_vep_96/  -->  11.50 GB\ndbfs:/databricks-datasets/genomics/grch38/  -->  1.57 GB\ndbfs:/databricks-datasets/genomics/grch38_merged_vep_96/  -->  13.91 GB\ndbfs:/databricks-datasets/genomics/grch38_refseq_vep_96/  -->  11.17 GB\ndbfs:/databricks-datasets/genomics/grch38_star/  -->  33.43 GB\ndbfs:/databricks-datasets/genomics/grch38_vep/  -->  14.67 GB\ndbfs:/databricks-datasets/genomics/grch38_vep_96/  -->  11.88 GB\ndbfs:/databricks-datasets/learning-spark-v2/sf-fire/  -->  1.07 GB\ndbfs:/databricks-datasets/med-images/camelyon16/  -->  109.81 GB\ndbfs:/databricks-datasets/sai-summit-2019-sf/  -->  1.06 GB\ndbfs:/databricks-datasets/timeseries/Fires/  -->  1.76 GB\ndbfs:/databricks-datasets/wiki/  -->  4.41 GB\n"
     ]
    }
   ],
   "source": [
    "# List only datasets larger than 1 GB\n",
    "def list_large_datasets(path=\"/databricks-datasets\", max_depth=2, current_depth=0):\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        for f in files:\n",
    "            if f.isDir():\n",
    "                # Compute folder size\n",
    "                try:\n",
    "                    sub_files = dbutils.fs.ls(f.path)\n",
    "                    total_size = sum(sf.size for sf in sub_files)\n",
    "                    size_gb = total_size / (1024 ** 3)\n",
    "                    if size_gb > 1:\n",
    "                        print(f\"{f.path}  -->  {size_gb:.2f} GB\")\n",
    "                    # Recurse deeper\n",
    "                    list_large_datasets(f.path, max_depth, current_depth + 1)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {path}: {e}\")\n",
    "\n",
    "# Run it\n",
    "list_large_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf51c10-71b0-4ce6-89b1-c508f990ee6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 5120231\nColumns: 34\nSize: 1.763 GB\n"
     ]
    }
   ],
   "source": [
    "# Simple Databricks cell: get dataset size and shape\n",
    "\n",
    "path = 'dbfs:/databricks-datasets/timeseries/Fires/'  \n",
    "\n",
    "df = spark.read.option(\"header\", True).csv(path)\n",
    "rows = df.count()\n",
    "cols = len(df.columns)\n",
    "\n",
    "# get total size in GB\n",
    "size_bytes = sum(f.size for f in dbutils.fs.ls(path))\n",
    "size_gb = size_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Rows: {rows}\")\n",
    "print(f\"Columns: {cols}\")\n",
    "print(f\"Size: {size_gb:.3f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b0161c-b9c1-45f4-91aa-d937f71ead52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Call Number: string (nullable = true)\n |-- Unit ID: string (nullable = true)\n |-- Incident Number: string (nullable = true)\n |-- Call Type: string (nullable = true)\n |-- Call Date: string (nullable = true)\n |-- Watch Date: string (nullable = true)\n |-- Received DtTm: string (nullable = true)\n |-- Entry DtTm: string (nullable = true)\n |-- Dispatch DtTm: string (nullable = true)\n |-- Response DtTm: string (nullable = true)\n |-- On Scene DtTm: string (nullable = true)\n |-- Transport DtTm: string (nullable = true)\n |-- Hospital DtTm: string (nullable = true)\n |-- Call Final Disposition: string (nullable = true)\n |-- Available DtTm: string (nullable = true)\n |-- Address: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Zipcode of Incident: string (nullable = true)\n |-- Battalion: string (nullable = true)\n |-- Station Area: string (nullable = true)\n |-- Box: string (nullable = true)\n |-- Original Priority: string (nullable = true)\n |-- Priority: string (nullable = true)\n |-- Final Priority: string (nullable = true)\n |-- ALS Unit: string (nullable = true)\n |-- Call Type Group: string (nullable = true)\n |-- Number of Alarms: string (nullable = true)\n |-- Unit Type: string (nullable = true)\n |-- Unit sequence in call dispatch: string (nullable = true)\n |-- Fire Prevention District: string (nullable = true)\n |-- Supervisor District: string (nullable = true)\n |-- Neighborhooods - Analysis Boundaries: string (nullable = true)\n |-- Location: string (nullable = true)\n |-- RowID: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60550072-463d-4d38-b2a8-cea8d29a3210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark:\n+--------------+-------------+--------------------+--------------------+\n|     Call Type|         City|       Response DtTm|       On Scene DtTm|\n+--------------+-------------+--------------------+--------------------+\n|        Alarms|San Francisco|10/18/2019 12:08:...|10/18/2019 12:11:...|\n|        Alarms|San Francisco|                NULL|                NULL|\n|        Alarms|San Francisco|                NULL|                NULL|\n|        Alarms|San Francisco|10/18/2019 12:09:...|10/18/2019 12:09:...|\n|Structure Fire|San Francisco|10/18/2019 12:13:...|10/18/2019 12:16:...|\n+--------------+-------------+--------------------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Select important columns from Fire Calls dataset\n",
    "df_spark_select = df.select(\n",
    "    \"Call Type\",\n",
    "    \"City\",\n",
    "    \"Response DtTm\",\n",
    "    \"On Scene DtTm\"\n",
    ")\n",
    "\n",
    "print(\"PySpark:\")\n",
    "df_spark_select.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ca9c0d-61c5-4bdc-9ff8-5684ba18c9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PySpark - Filtered Fire Calls (Structure Fires in San Francisco):\n+--------------+-------------+----------------------+----------------------+\n|Call Type     |City         |Response DtTm         |On Scene DtTm         |\n+--------------+-------------+----------------------+----------------------+\n|Structure Fire|San Francisco|10/18/2019 12:13:52 AM|10/18/2019 12:16:16 AM|\n|Structure Fire|San Francisco|10/18/2019 12:14:28 AM|NULL                  |\n|Structure Fire|San Francisco|10/18/2019 02:24:56 AM|10/18/2019 02:28:08 AM|\n|Structure Fire|San Francisco|09/17/2017 10:06:29 AM|NULL                  |\n|Structure Fire|San Francisco|10/18/2019 07:08:33 AM|NULL                  |\n+--------------+-------------+----------------------+----------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Apply multiple filters on Fire Calls dataset\n",
    "df_filtered = (\n",
    "    df_spark_select\n",
    "    # Filter 1: Keep only rows where Call Type is \"Structure Fire\"\n",
    "    .filter(col(\"Call Type\") == \"Structure Fire\")\n",
    "    # Filter 2: Keep only rows where City is \"San Francisco\"\n",
    "    .filter(col(\"City\") == \"San Francisco\")\n",
    ")\n",
    "\n",
    "print(\"✅ PySpark - Filtered Fire Calls (Structure Fires in San Francisco):\")\n",
    "df_filtered.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13766fe9-fa14-40fc-a948-162a534dd953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----------+------------------+------------------+------------------+--------------------+------------------+\n|Call Type                         |count_calls|avg_delay_min     |median_delay_min  |p95_delay_min     |min_delay_min       |max_delay_min     |\n+----------------------------------+-----------+------------------+------------------+------------------+--------------------+------------------+\n|Administrative                    |66         |27.54848484848485 |14.033333333333333|90.25             |0.05                |93.16666666666667 |\n|Mutual Aid / Assist Outside Agency|173        |28.03150289017341 |12.616666666666667|77.4              |0.03333333333333333 |265.25            |\n|Aircraft Emergency                |507        |19.527843523997372|14.35             |49.81666666666667 |0.05                |119.06666666666666|\n|Watercraft in Distress            |457        |9.325054704595184 |5.916666666666667 |30.25             |0.13333333333333333 |116.51666666666667|\n|Train / Rail Incident             |885        |7.62864406779661  |4.316666666666666 |26.833333333333332|0.016666666666666666|116.1             |\n|Water Rescue                      |12277      |8.685153810648638 |6.0               |24.25             |0.016666666666666666|226.96666666666667|\n|Suspicious Package                |220        |7.390757575757576 |4.8               |22.75             |0.016666666666666666|43.15             |\n|High Angle Rescue                 |788        |9.081641285956005 |5.966666666666667 |20.833333333333332|0.03333333333333333 |388.3666666666667 |\n|HazMat                            |2741       |6.398279216830841 |3.933333333333333 |20.15             |0.016666666666666666|109.56666666666666|\n|Assist Police                     |888        |6.024455705705705 |3.533333333333333 |17.916666666666668|0.016666666666666666|168.73333333333332|\n+----------------------------------+-----------+------------------+------------------+------------------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Aggregation on the Fire Calls data \n",
    "from pyspark.sql.functions import col, to_timestamp, avg, count, min as min_, max as max_, desc, expr\n",
    "\n",
    "TS_FMT = \"M/d/yyyy h:mm:ss a\"\n",
    "\n",
    "# 1) Add response delay (minutes) from the selected columns\n",
    "df_with_delay = (\n",
    "    df_spark_select\n",
    "    .withColumn(\"received_ts\", to_timestamp(col(\"Response DtTm\"), TS_FMT))\n",
    "    .withColumn(\"on_scene_ts\", to_timestamp(col(\"On Scene DtTm\"), TS_FMT))\n",
    "    .withColumn(\"response_delay_min\", (col(\"on_scene_ts\").cast(\"long\") - col(\"received_ts\").cast(\"long\")) / 60.0)\n",
    ")\n",
    "\n",
    "# 2) Aggregation per Call Type, compute count, avg, median, p95, min, max delays\n",
    "agg_result = (\n",
    "    df_with_delay\n",
    "    .filter(col(\"response_delay_min\").isNotNull() & (col(\"response_delay_min\") > 0))\n",
    "    .groupBy(\"Call Type\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count_calls\"),\n",
    "        avg(\"response_delay_min\").alias(\"avg_delay_min\"),\n",
    "        expr(\"percentile_approx(response_delay_min, 0.5)\").alias(\"median_delay_min\"),\n",
    "        expr(\"percentile_approx(response_delay_min, 0.95)\").alias(\"p95_delay_min\"),\n",
    "        min_(\"response_delay_min\").alias(\"min_delay_min\"),\n",
    "        max_(\"response_delay_min\").alias(\"max_delay_min\"),\n",
    "    )\n",
    "    .orderBy(desc(\"p95_delay_min\"))\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "agg_result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69c01e41-e93b-489c-97ea-3125bc98fbf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PySpark - GroupBy with Aggregation:\n+-----------------------------------+----------------------+-----------+\n|Call Type                          |avg_response_delay_min|count_calls|\n+-----------------------------------+----------------------+-----------+\n|Mutual Aid / Assist Outside Agency |28.03150289017341     |173        |\n|Administrative                     |27.54848484848485     |66         |\n|Aircraft Emergency                 |19.527843523997372    |507        |\n|Watercraft in Distress             |9.325054704595184     |457        |\n|High Angle Rescue                  |9.081641285956005     |788        |\n|Water Rescue                       |8.685153810648638     |12277      |\n|Train / Rail Incident              |7.62864406779661      |885        |\n|Suspicious Package                 |7.390757575757576     |220        |\n|Marine Fire                        |6.943526785714285     |224        |\n|Confined Space / Structure Collapse|6.7111986001749795    |381        |\n+-----------------------------------+----------------------+-----------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, count, desc, to_timestamp\n",
    "\n",
    "# GroupBy with aggregation on Fire Calls dataset\n",
    "TS_FMT = \"M/d/yyyy h:mm:ss a\"\n",
    "\n",
    "# Add response delay in minutes\n",
    "df_with_delay = (\n",
    "    df_spark_select\n",
    "    .withColumn(\"received_ts\", to_timestamp(col(\"Response DtTm\"), TS_FMT))\n",
    "    .withColumn(\"on_scene_ts\", to_timestamp(col(\"On Scene DtTm\"), TS_FMT))\n",
    "    .withColumn(\"response_delay_min\", (col(\"on_scene_ts\").cast(\"long\") - col(\"received_ts\").cast(\"long\")) / 60.0)\n",
    ")\n",
    "\n",
    "# Group by Call Type and calculate average delay and count of calls\n",
    "df_grouped = (\n",
    "    df_with_delay\n",
    "    .filter(col(\"response_delay_min\").isNotNull() & (col(\"response_delay_min\") > 0))\n",
    "    .groupBy(\"Call Type\")\n",
    "    .agg(\n",
    "        avg(\"response_delay_min\").alias(\"avg_response_delay_min\"),\n",
    "        count(\"*\").alias(\"count_calls\")\n",
    "    )\n",
    "    .orderBy(desc(\"avg_response_delay_min\"))\n",
    ")\n",
    "\n",
    "print(\"✅ PySpark - GroupBy with Aggregation:\")\n",
    "df_grouped.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52f3dee-f3b3-4d62-bdd6-f5cf8d550483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>day_of_week</th><th>total_calls</th></tr></thead><tbody><tr><td>Friday</td><td>695073</td></tr><tr><td>Saturday</td><td>694020</td></tr><tr><td>Sunday</td><td>667572</td></tr><tr><td>Thursday</td><td>666488</td></tr><tr><td>Monday</td><td>666011</td></tr><tr><td>Wednesday</td><td>661494</td></tr><tr><td>Tuesday</td><td>656739</td></tr><tr><td>null</td><td>412834</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Friday",
         695073
        ],
        [
         "Saturday",
         694020
        ],
        [
         "Sunday",
         667572
        ],
        [
         "Thursday",
         666488
        ],
        [
         "Monday",
         666011
        ],
        [
         "Wednesday",
         661494
        ],
        [
         "Tuesday",
         656739
        ],
        [
         null,
         412834
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "day_of_week",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_calls",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 63
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "day_of_week",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_calls",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, round, upper, concat_ws\n",
    "\n",
    "# Column transformations using withColumn on Fire Calls dataset\n",
    "TS_FMT = \"M/d/yyyy h:mm:ss a\"\n",
    "\n",
    "df_transformed = (\n",
    "    df_spark_select\n",
    "    # Convert timestamps\n",
    "    .withColumn(\"received_ts\", to_timestamp(col(\"Response DtTm\"), TS_FMT))\n",
    "    .withColumn(\"on_scene_ts\", to_timestamp(col(\"On Scene DtTm\"), TS_FMT))\n",
    "    # Compute new column: response delay in minutes\n",
    "    .withColumn(\"response_delay_min\", round((col(\"on_scene_ts\").cast(\"long\") - col(\"received_ts\").cast(\"long\")) / 60.0, 2))\n",
    "    # Uppercase city name\n",
    "    .withColumn(\"City_Upper\", upper(col(\"City\")))\n",
    "    # Combine city and call type into one string\n",
    "    .withColumn(\"Call_Info\", concat_ws(\" - \", col(\"City\"), col(\"Call Type\")))\n",
    ")\n",
    "\n",
    "print(\"✅ PySpark - Column Transformations using withColumn:\")\n",
    "df_transformed.select(\"City\", \"Call Type\", \"response_delay_min\", \"City_Upper\", \"Call_Info\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "476a8b94-8a1c-4b7c-8144-d3ef8570cbdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>City</th><th>slow_calls</th></tr></thead><tbody><tr><td>San Francisco</td><td>196912</td></tr><tr><td>SF</td><td>181511</td></tr><tr><td>SAN FRANCISCO</td><td>4589</td></tr><tr><td>TI</td><td>2503</td></tr><tr><td>Treasure Isla</td><td>1608</td></tr><tr><td>Presidio</td><td>1101</td></tr><tr><td>PR</td><td>799</td></tr><tr><td>null</td><td>784</td></tr><tr><td>SFO</td><td>524</td></tr><tr><td>Yerba Buena</td><td>302</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "San Francisco",
         196912
        ],
        [
         "SF",
         181511
        ],
        [
         "SAN FRANCISCO",
         4589
        ],
        [
         "TI",
         2503
        ],
        [
         "Treasure Isla",
         1608
        ],
        [
         "Presidio",
         1101
        ],
        [
         "PR",
         799
        ],
        [
         null,
         784
        ],
        [
         "SFO",
         524
        ],
        [
         "Yerba Buena",
         302
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "City",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "slow_calls",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 57
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "slow_calls",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Assumes a temp view named `fire_calls` exists with columns:\n",
    "-- `Call Type`, City, `Response DtTm`, `On Scene DtTm`\n",
    "\n",
    "-- Query 1: Average response delay per Call Type & City (parses timestamps inline)\n",
    "SELECT\n",
    "  `Call Type`,\n",
    "  City,\n",
    "  AVG(\n",
    "    (unix_timestamp(`On Scene DtTm`, 'M/d/yyyy h:mm:ss a') -\n",
    "     unix_timestamp(`Response DtTm`, 'M/d/yyyy h:mm:ss a')) / 60.0\n",
    "  ) AS avg_delay_min,\n",
    "  COUNT(*) AS count_calls\n",
    "FROM fire_calls\n",
    "GROUP BY `Call Type`, City\n",
    "ORDER BY avg_delay_min DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- Query 2: Count of \"slow\" responses (>10 min) per City (again computed inline)\n",
    "SELECT\n",
    "  City,\n",
    "  COUNT(*) AS slow_calls\n",
    "FROM fire_calls\n",
    "WHERE\n",
    "  (unix_timestamp(`On Scene DtTm`, 'M/d/yyyy h:mm:ss a') -\n",
    "   unix_timestamp(`Response DtTm`, 'M/d/yyyy h:mm:ss a')) / 60.0 > 10\n",
    "GROUP BY City\n",
    "ORDER BY slow_calls DESC\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67dd4d80-f3a4-42d3-bb2a-609a1f4eb642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>city</th><th>slow_calls</th></tr></thead><tbody><tr><td>San Francisco</td><td>196912</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "San Francisco",
         196912
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "city",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "slow_calls",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 79
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "slow_calls",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- ✅ OPTIMIZED (works with the columns: uses Response DtTm, not Received DtTm)\n",
    "-- Assumes a temp view `fire_calls` with: `Call Type`, City, `Response DtTm`, `On Scene DtTm`\n",
    "\n",
    "-- 0) Stage: precompute delay & apply EARLY filters (narrow columns)\n",
    "CREATE OR REPLACE TEMP VIEW fire_calls_stage AS\n",
    "SELECT\n",
    "  `Call Type` AS call_type,\n",
    "  City        AS city,\n",
    "  (\n",
    "    unix_timestamp(`On Scene DtTm`, 'M/d/yyyy h:mm:ss a') -\n",
    "    unix_timestamp(`Response DtTm`,  'M/d/yyyy h:mm:ss a')\n",
    "  ) / 60.0 AS delay_min\n",
    "FROM fire_calls\n",
    "WHERE City = 'San Francisco'         -- early filter\n",
    "  AND `Call Type` IS NOT NULL;       -- drop null group keys\n",
    "\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW fire_calls_stage_part AS\n",
    "SELECT /*+ REPARTITION(16, call_type) */  -- tune 16 to your cluster size\n",
    "  call_type, city, delay_min\n",
    "FROM fire_calls_stage\n",
    "WHERE delay_min > 0;                  -- prune before wide ops\n",
    "\n",
    "-- Query A: Average delay per Call Type (aggregate after pruning)\n",
    "SELECT\n",
    "  call_type,\n",
    "  COUNT(*)                 AS count_calls,\n",
    "  ROUND(AVG(delay_min),2)  AS avg_delay_min\n",
    "FROM fire_calls_stage_part\n",
    "GROUP BY call_type\n",
    "ORDER BY avg_delay_min DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- Query B: Count of \"slow\" responses (>10 min) per City\n",
    "SELECT\n",
    "  city,\n",
    "  COUNT(*) AS slow_calls\n",
    "FROM fire_calls_stage_part\n",
    "WHERE delay_min > 10\n",
    "GROUP BY city\n",
    "ORDER BY slow_calls DESC\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a06a06ea-d2c8-4797-9d6b-70f1f6a0cf45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PySpark — Optimized (filters early + partition by key + minimal shuffles, no persist):\n+----------------------------------+------------------+-----------+\n|Call Type                         |avg_delay_min     |count_calls|\n+----------------------------------+------------------+-----------+\n|Mutual Aid / Assist Outside Agency|86.18238095238097 |21         |\n|High Angle Rescue                 |11.198544061302687|261        |\n|Watercraft in Distress            |11.052735849056605|106        |\n|Water Rescue                      |8.801310844464568 |4417       |\n|Train / Rail Incident             |8.619045801526717 |262        |\n|Suspicious Package                |7.456808510638298 |47         |\n|Marine Fire                       |7.300263157894739 |38         |\n|Medical Incident                  |6.5862889263119415|921003     |\n|Other                             |5.812180554645021 |15253      |\n|Train / Rail Fire                 |5.754761904761905 |21         |\n+----------------------------------+------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, count, desc\n",
    "\n",
    "# (City/Call Type filtered, response_delay_min computed, and > 0)\n",
    "\n",
    "# 1) Keep only columns needed before wide ops (narrow -> fewer bytes to shuffle)\n",
    "df_narrow = df_clean.select(\"Call Type\", \"response_delay_min\")\n",
    "\n",
    "# 2) Partition by the group key to reduce shuffle during the aggregation\n",
    "df_partitioned = df_narrow.repartition(16, \"Call Type\")  # adjust 16 to your cluster size\n",
    "\n",
    "# 3) Single wide op: groupBy + agg \n",
    "df_agg = (\n",
    "    df_partitioned\n",
    "    .groupBy(\"Call Type\")\n",
    "    .agg(\n",
    "        avg(\"response_delay_min\").alias(\"avg_delay_min\"),\n",
    "        count(\"*\").alias(\"count_calls\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4) Global sort only after aggregation (on many fewer rows)\n",
    "top10 = df_agg.orderBy(desc(\"avg_delay_min\")).limit(10)\n",
    "\n",
    "print(\"✅ PySpark — Optimized (filters early + partition by key + minimal shuffles, no persist):\")\n",
    "top10.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2459a3ad-6a51-4190-956f-39700c07eb2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available catalogs:\n+---------+\n|catalog  |\n+---------+\n|samples  |\n|system   |\n|workspace|\n+---------+\n\ncurrent_catalog=workspace, current_schema=default\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Discover catalogs/schemas available in this workspace ---\n",
    "print(\"Available catalogs:\")\n",
    "spark.sql(\"SHOW CATALOGS\").show(truncate=False)\n",
    "\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
    "current_schema  = spark.sql(\"SELECT current_schema()\").first()[0]\n",
    "print(f\"current_catalog={current_catalog}, current_schema={current_schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97963de-97db-4dd1-bfcf-6ab6c0c986d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully wrote results to: /Volumes/workspace/default/analytics_vol/fire_calls_top10_parquet\n"
     ]
    }
   ],
   "source": [
    "# ✅ Use the workspace catalog and default schema (supported on serverless)\n",
    "spark.sql(\"USE CATALOG workspace\")\n",
    "spark.sql(\"USE SCHEMA default\")\n",
    "\n",
    "# Create a Volume once (safe to re-run)\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS analytics_vol COMMENT 'Analysis outputs'\")\n",
    "\n",
    "# Write your results to the Volume \n",
    "dest_path = \"/Volumes/workspace/default/analytics_vol/fire_calls_top10_parquet\"\n",
    "\n",
    "# Save as Parquet \n",
    "top10.write.mode(\"overwrite\").parquet(dest_path)\n",
    "\n",
    "print(\"✅ Successfully wrote results to:\", dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3129f88-0362-4d9f-b6a3-f69c5de071a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations built in 0.0007s (no Spark job yet)\n\nAction 1: count()\ncount() = 1,658,598 rows, took 3.55s\n\nAction 2: show()\n+--------------+-------------+------------------+\n|Call Type     |City         |response_delay_min|\n+--------------+-------------+------------------+\n|Alarms        |San Francisco|2.7               |\n|Alarms        |San Francisco|NULL              |\n|Alarms        |San Francisco|NULL              |\n|Alarms        |San Francisco|0.0               |\n|Structure Fire|San Francisco|2.4               |\n+--------------+-------------+------------------+\nonly showing top 5 rows\nshow() took 0.27s\n\n\uD83D\uDD0D Each action re-executes the transformations unless you materialize it.\n\nAction 3 (after local checkpoint): show()\n+--------------+-------------+------------------+\n|Call Type     |City         |response_delay_min|\n+--------------+-------------+------------------+\n|Alarms        |San Francisco|2.7               |\n|Alarms        |San Francisco|NULL              |\n|Alarms        |San Francisco|NULL              |\n|Alarms        |San Francisco|0.0               |\n|Structure Fire|San Francisco|2.4               |\n+--------------+-------------+------------------+\nonly showing top 5 rows\nshow() after local checkpoint took 0.56s\n"
     ]
    }
   ],
   "source": [
    "# Actions vs Transformations \n",
    "from pyspark.sql.functions import col, to_timestamp, round\n",
    "import time\n",
    "\n",
    "TS_FMT = \"M/d/yyyy h:mm:ss a\"\n",
    "df = df_spark_select\n",
    "\n",
    "# ---- Transformations (lazy) ----\n",
    "t0 = time.time()\n",
    "t = (\n",
    "    df\n",
    "    .filter(col(\"City\") == \"San Francisco\")\n",
    "    .filter(col(\"Call Type\").isNotNull())\n",
    "    .withColumn(\"received_ts\", to_timestamp(col(\"Response DtTm\"), TS_FMT))\n",
    "    .withColumn(\"on_scene_ts\", to_timestamp(col(\"On Scene DtTm\"), TS_FMT))\n",
    "    .withColumn(\n",
    "        \"response_delay_min\",\n",
    "        round((col(\"on_scene_ts\").cast(\"long\") - col(\"received_ts\").cast(\"long\")) / 60.0, 2)\n",
    "    )\n",
    "    .select(\"Call Type\", \"City\", \"response_delay_min\")\n",
    ")\n",
    "print(f\"Transformations built in {time.time() - t0:.4f}s (no Spark job yet)\")\n",
    "\n",
    "# ---- Actions (eager) ----\n",
    "print(\"\\nAction 1: count()\")\n",
    "t1 = time.time()\n",
    "cnt = t.count()     # triggers a job\n",
    "print(f\"count() = {cnt:,} rows, took {time.time() - t1:.2f}s\")\n",
    "\n",
    "print(\"\\nAction 2: show()\")\n",
    "t2 = time.time()\n",
    "t.show(5, truncate=False)  # triggers another job \n",
    "print(f\"show() took {time.time() - t2:.2f}s\")\n",
    "\n",
    "print(\"\\n\uD83D\uDD0D Each action re-executes the transformations unless you materialize it.\")\n",
    "\n",
    "# localCheckpoint() breaks lineage and materializes the result in executor memory/disk\n",
    "# without using metastore features that are blocked on serverless.\n",
    "t_ckpt = t.localCheckpoint(eager=True)\n",
    "\n",
    "print(\"\\nAction 3 (after local checkpoint): show()\")\n",
    "t3 = time.time()\n",
    "t_ckpt.show(5, truncate=False)  # should avoid recomputing full upstream lineage\n",
    "print(f\"show() after local checkpoint took {time.time() - t3:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00fb421-b8fd-419d-9e33-7786a9f1afaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test AUC: 0.6871\n✅ Test F1 : 0.8763\n+------------------+----------+------------------+-------------+----------+-----------------------------------------+\n|Call Type         |City      |response_delay_min|slow_response|prediction|probability                              |\n+------------------+----------+------------------+-------------+----------+-----------------------------------------+\n|Aircraft Emergency|SFO       |NULL              |0            |0.0       |[0.5992085342183672,0.4007914657816328]  |\n|Aircraft Emergency|SFO       |12.6              |1            |0.0       |[0.5875709822564589,0.4124290177435411]  |\n|Alarms            |DC        |NULL              |0            |0.0       |[0.98835964573658,0.011640354263420027]  |\n|Alarms            |Fort Mason|5.75              |0            |0.0       |[0.9806818508422595,0.01931814915774055] |\n|Alarms            |Fort Mason|4.633333333333334 |0            |0.0       |[0.9807713700576458,0.019228629942354192]|\n|Alarms            |Fort Mason|6.35              |0            |0.0       |[0.9807713700576458,0.019228629942354192]|\n|Alarms            |Fort Mason|3.9166666666666665|0            |0.0       |[0.9807713700576458,0.019228629942354192]|\n|Alarms            |Fort Mason|4.883333333333334 |0            |0.0       |[0.980690821117373,0.019309178882626954] |\n|Alarms            |Fort Mason|5.266666666666667 |0            |0.0       |[0.980037387363936,0.019962612636064025] |\n|Alarms            |Fort Mason|4.35              |0            |0.0       |[0.9805633327841002,0.01943666721589976] |\n+------------------+----------+------------------+-------------+----------+-----------------------------------------+\nonly showing top 10 rows\n\nConfusion matrix (prediction vs label):\n+-------------+----------+------+\n|slow_response|prediction| count|\n+-------------+----------+------+\n|            0|       0.0|860653|\n|            0|       1.0|     1|\n|            1|       0.0| 78584|\n|            1|       1.0|     8|\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# MLlib — Binary Classification on Fire Calls (serverless-safe)\n",
    "from pyspark.sql.functions import col, when, to_timestamp, hour, dayofweek\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "TS_FMT = \"M/d/yyyy h:mm:ss a\"\n",
    "\n",
    "# Use your selected columns DataFrame from earlier\n",
    "df_src = df_spark_select\n",
    "\n",
    "# Pick the available \"start\" timestamp column\n",
    "start_ts_col = \"Received DtTm\" if \"Received DtTm\" in df_src.columns else \"Response DtTm\"\n",
    "\n",
    "# 1) Prepare label & features\n",
    "df_ml = (\n",
    "    df_src\n",
    "    .withColumn(\"start_ts\", to_timestamp(col(start_ts_col), TS_FMT))\n",
    "    .withColumn(\"on_scene_ts\", to_timestamp(col(\"On Scene DtTm\"), TS_FMT))\n",
    "    .withColumn(\"response_delay_min\", (col(\"on_scene_ts\").cast(\"long\") - col(\"start_ts\").cast(\"long\"))/60.0)\n",
    "    .withColumn(\"slow_response\", when(col(\"response_delay_min\") >= 10, 1).otherwise(0))\n",
    "    .withColumn(\"hr\", hour(col(\"start_ts\")))\n",
    "    .withColumn(\"dow\", dayofweek(col(\"start_ts\")))\n",
    "    .dropna(subset=[\"slow_response\", \"hr\", \"dow\", \"City\", \"Call Type\"])\n",
    ")\n",
    "\n",
    "# Categorical -> index -> one-hot\n",
    "cat_cols = [\"Call Type\", \"City\"]\n",
    "idx_cols = [c + \"_idx\" for c in cat_cols]\n",
    "ohe_cols = [c + \"_vec\" for c in cat_cols]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=i, handleInvalid=\"keep\") for c, i in zip(cat_cols, idx_cols)]\n",
    "encoder  = OneHotEncoder(inputCols=idx_cols, outputCols=ohe_cols)\n",
    "assembler = VectorAssembler(inputCols=ohe_cols + [\"hr\", \"dow\"], outputCol=\"features\")\n",
    "\n",
    "# 2) Model\n",
    "lr = LogisticRegression(labelCol=\"slow_response\", featuresCol=\"features\", maxIter=50)\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler, lr])\n",
    "\n",
    "# 3) Train / Test\n",
    "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train_df)\n",
    "pred  = model.transform(test_df)\n",
    "\n",
    "# 4) Evaluate\n",
    "auc = BinaryClassificationEvaluator(labelCol=\"slow_response\", rawPredictionCol=\"rawPrediction\").evaluate(pred)\n",
    "f1  = MulticlassClassificationEvaluator(labelCol=\"slow_response\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(pred)\n",
    "print(f\"✅ Test AUC: {auc:.4f}\")\n",
    "print(f\"✅ Test F1 : {f1:.4f}\")\n",
    "\n",
    "# Inspect\n",
    "pred.select(\"Call Type\",\"City\",\"response_delay_min\",\"slow_response\",\"prediction\",\"probability\").show(10, truncate=False)\n",
    "print(\"\\nConfusion matrix (prediction vs label):\")\n",
    "(pred.groupBy(\"slow_response\",\"prediction\").count().orderBy(\"slow_response\",\"prediction\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b664b9-d1ef-4447-85ce-bc1ee477f602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonTopK(sortOrder=[avg_delay_min#19360 DESC NULLS LAST], partitionOrderCount=0)\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#16709]\n               +- PhotonShuffleExchangeSink SinglePartition\n                  +- PhotonTopK(sortOrder=[avg_delay_min#19360 DESC NULLS LAST], partitionOrderCount=0)\n                     +- PhotonGroupingAgg(keys=[Call Type#15921], functions=[avg(response_delay_min#19359), count(1)])\n                        +- PhotonShuffleExchangeSource\n                           +- PhotonShuffleMapStage REPARTITION_BY_NUM, [id=#16701]\n                              +- PhotonShuffleExchangeSink hashpartitioning(Call Type#15921, 16)\n                                 +- PhotonProject [Call Type#15921, round((cast((cast(gettimestamp(On Scene DtTm#15928, M/d/yyyy h:mm:ss a, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as bigint) - cast(gettimestamp(Response DtTm#15927, M/d/yyyy h:mm:ss a, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as bigint)) as double) / 60.0), 2) AS response_delay_min#19359]\n                                    +- PhotonFilter (((isnotnull(City#15934) AND isnotnull(Call Type#15921)) AND (City#15934 = San Francisco)) AND (round((cast((cast(gettimestamp(On Scene DtTm#15928, M/d/yyyy h:mm:ss a, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as bigint) - cast(gettimestamp(Response DtTm#15927, M/d/yyyy h:mm:ss a, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as bigint)) as double) / 60.0), 2) > 0.0))\n                                       +- PhotonRowToColumnar\n                                          +- FileScan csv [Call Type#15921,Response DtTm#15927,On Scene DtTm#15928,City#15934] Batched: false, DataFilters: [isnotnull(City#15934), isnotnull(Call Type#15921), (City#15934 = San Francisco), (round((cast((c..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/timeseries/Fires], PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(Call Type), EqualTo(City,San Francisco)], ReadSchema: struct<Call Type:string,Response DtTm:string,On Scene DtTm:string,City:string>\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "top10.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8260fb6f-ba47-497d-8ef3-9a9a09be5dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----------+\n|           Call Type|     avg_delay_min|count_calls|\n+--------------------+------------------+-----------+\n|Mutual Aid / Assi...| 86.18238095238097|         21|\n|   High Angle Rescue|11.198544061302687|        261|\n|Watercraft in Dis...|11.052735849056605|        106|\n|        Water Rescue| 8.801310844464568|       4417|\n|Train / Rail Inci...| 8.619045801526717|        262|\n|  Suspicious Package| 7.456808510638298|         47|\n|         Marine Fire| 7.300263157894739|         38|\n|    Medical Incident|6.5862889263119415|     921003|\n|               Other| 5.812180554645021|      15253|\n|   Train / Rail Fire| 5.754761904761905|         21|\n+--------------------+------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "top10.limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07e3106-268a-4a40-aa28-d8ca91be03ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (no materialization) - first count:  4.74s\nBaseline (no materialization) - second count: 4.12s\n\nMaterialize with localCheckpoint(eager=True): 4.99s\nAfter checkpoint - first count:  0.24s\nAfter checkpoint - second count: 0.12s\n+----------------------------------+------------------+-----------+\n|Call Type                         |avg_delay_min     |count_calls|\n+----------------------------------+------------------+-----------+\n|Mutual Aid / Assist Outside Agency|86.18238095238097 |21         |\n|High Angle Rescue                 |11.198544061302687|261        |\n|Watercraft in Distress            |11.052735849056605|106        |\n|Water Rescue                      |8.801310844464568 |4417       |\n|Train / Rail Incident             |8.619045801526717 |262        |\n+----------------------------------+------------------+-----------+\nonly showing top 5 rows\nshow() from checkpoint took: 0.22s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "df_src = top10  \n",
    "\n",
    "\n",
    "start = time.time()\n",
    "_ = df_src.count()                 # first action (full recompute)\n",
    "t1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = df_src.count()                 # second action (recomputes again)\n",
    "t2 = time.time() - start\n",
    "\n",
    "print(f\"Baseline (no materialization) - first count:  {t1:.2f}s\")\n",
    "print(f\"Baseline (no materialization) - second count: {t2:.2f}s\")\n",
    "\n",
    "# localCheckpoint() \n",
    "start = time.time()\n",
    "df_ckpt = df_src.localCheckpoint(eager=True)  # materialize now\n",
    "t3 = time.time() - start\n",
    "print(f\"\\nMaterialize with localCheckpoint(eager=True): {t3:.2f}s\")\n",
    "\n",
    "# First action after checkpoint \n",
    "start = time.time()\n",
    "_ = df_ckpt.count()\n",
    "t4 = time.time() - start\n",
    "\n",
    "# Second action after checkpoint \n",
    "start = time.time()\n",
    "_ = df_ckpt.count()\n",
    "t5 = time.time() - start\n",
    "\n",
    "print(f\"After checkpoint - first count:  {t4:.2f}s\")\n",
    "print(f\"After checkpoint - second count: {t5:.2f}s\")\n",
    "\n",
    "start = time.time()\n",
    "df_ckpt.show(5, truncate=False)\n",
    "print(f\"show() from checkpoint took: {time.time() - start:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5710242b-b167-43d5-b04d-ffd1fcd9cb0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Starting PySpark Fire Calls pipeline...\n✅ PySpark - Optimized Query (Filters Early in Pipeline):\n+----------------------------------+------------------+-----------+\n|Call Type                         |avg_delay_min     |count_calls|\n+----------------------------------+------------------+-----------+\n|Mutual Aid / Assist Outside Agency|86.18238095238095 |21         |\n|High Angle Rescue                 |11.198544061302682|261        |\n|Watercraft in Distress            |11.052735849056605|106        |\n|Water Rescue                      |8.801310844464572 |4417       |\n|Train / Rail Incident             |8.619045801526717 |262        |\n|Suspicious Package                |7.456808510638298 |47         |\n|Marine Fire                       |7.300263157894736 |38         |\n|Medical Incident                  |6.586288926311867 |921003     |\n|Other                             |5.812180554644988 |15253      |\n|Train / Rail Fire                 |5.754761904761905 |21         |\n+----------------------------------+------------------+-----------+\nonly showing top 10 rows\n\n✅ Successfully wrote results to: /Volumes/workspace/default/analytics_vol/fire_calls_top10_parquet\n\n✅ Output successfully reloaded:\n+----------------------------------+------------------+-----------+\n|Call Type                         |avg_delay_min     |count_calls|\n+----------------------------------+------------------+-----------+\n|Mutual Aid / Assist Outside Agency|86.18238095238095 |21         |\n|High Angle Rescue                 |11.198544061302682|261        |\n|Watercraft in Distress            |11.052735849056605|106        |\n|Water Rescue                      |8.801310844464572 |4417       |\n|Train / Rail Incident             |8.619045801526717 |262        |\n+----------------------------------+------------------+-----------+\nonly showing top 5 rows\n\n\uD83C\uDF89 Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, avg, count, desc, round\n",
    "\n",
    "# ================================\n",
    "# ✅ FINAL PIPELINE EXECUTION CELL\n",
    "# ================================\n",
    "\n",
    "print(\"\uD83D\uDE80 Starting PySpark Fire Calls pipeline...\")\n",
    "\n",
    "TS_FMT = \"M/d/yyyy h:mm:ss a\"\n",
    "\n",
    "# Step 1 — Filter early to minimize processing\n",
    "df_filtered = (\n",
    "    df\n",
    "    .filter(col(\"City\") == \"San Francisco\")\n",
    "    .filter(col(\"Call Type\").isNotNull())\n",
    ")\n",
    "\n",
    "# Step 2 — Add timestamp columns and compute response delay\n",
    "df_with_delay = (\n",
    "    df_filtered\n",
    "    .withColumn(\"received_ts\", to_timestamp(col(\"Response DtTm\"), TS_FMT))\n",
    "    .withColumn(\"on_scene_ts\", to_timestamp(col(\"On Scene DtTm\"), TS_FMT))\n",
    "    .withColumn(\n",
    "        \"response_delay_min\",\n",
    "        round((col(\"on_scene_ts\").cast(\"long\") - col(\"received_ts\").cast(\"long\")) / 60.0, 2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3 — Keep only positive delays\n",
    "df_clean = df_with_delay.filter(col(\"response_delay_min\") > 0)\n",
    "\n",
    "# Step 4 — Aggregate by Call Type\n",
    "df_result = (\n",
    "    df_clean\n",
    "    .groupBy(\"Call Type\")\n",
    "    .agg(\n",
    "        avg(\"response_delay_min\").alias(\"avg_delay_min\"),\n",
    "        count(\"*\").alias(\"count_calls\")\n",
    "    )\n",
    "    .orderBy(desc(\"avg_delay_min\"))\n",
    ")\n",
    "\n",
    "print(\"✅ PySpark - Optimized Query (Filters Early in Pipeline):\")\n",
    "df_result.show(10, truncate=False)\n",
    "\n",
    "# Step 5 — Save the result as Parquet in your Databricks Volume\n",
    "dest_path = \"/Volumes/workspace/default/analytics_vol/fire_calls_top10_parquet\"\n",
    "df_result.write.mode(\"overwrite\").parquet(dest_path)\n",
    "\n",
    "print(\"\\n✅ Successfully wrote results to:\", dest_path)\n",
    "\n",
    "# Step 6 — Read back the saved data to confirm output\n",
    "check_df = spark.read.parquet(dest_path)\n",
    "print(\"\\n✅ Output successfully reloaded:\")\n",
    "check_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n\uD83C\uDF89 Pipeline completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5989310494657433,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Week11_Pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}